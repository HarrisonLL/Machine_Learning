Part I. ML methods


- Linear Regression & Penalized Linear Regression
    idea: OLS: objective is to minimize the sum of squared errors
          variants: handling outliers: Robust linear regression
                    overfitting: L1 (Lasso) Regression, L2 (Ridge) Regression
    details to note: 
          L1 (Lasso) could be used as feature selection since it can shrink coeffiencts to 0
          L2 is usually better than L1 in terms of performance
    pros and cons:
          pros: linear regression: can be used as baseline model
                Interpretable


- Logistic Regression
    idea: transform a binary outcome to a continuous probability


- KNN
    idea: memorize the entire training set, and then for a test point find its k-nearest  neighbor and take
          the majority vote
    
    pros and cons:
        pros: simple and no assumptions about the data
        cons: memory inefficient: need to store the entire training set
	      often fails if dimension (d) is large

- Naive Bayes
    idea: use Bayes rules to find posterior probability distribution
	  derive the class conditional distribution through multiplication of probs in training sample

    pros and cons: 
        pros: works well in high dimension (such as text classification)
        cons: assumes independence of each predictors

- Linear Discriminant analysis
    idea: use Bayes rules to find posterior probability distribution
	  derive the class conditional distribution through Gaussian distribution (so it becomes the assumption of distribution of training data)


-- Tree-based model--

- Random Forest:
    ensemble method;
    take average of prediction outcomes from a set of DTs:
            - regression: average
            - classification: majority vote
    pros and cons:
        - pros: generally good performance due to not correlated trees
        - cons: could be black-box model

    
- Boosting:
        trees and weights are learned sequentially (samples are downweighted or unweighted )
	final vote is * weighted average


    details:
        - downweight: if samples are weighted correctly
          upweight: if samples are weighted incorrectly
        - a set of weak learners that collaboratively becomes an effective learner after ensembling

    - Gradient Boost:
        To avoid overfitting on gradient, a new weaker learner f_t is introduced:
            The f_t is added to the negative of the gradient of F_t-1 to get the loss.
            Minimize the loss to get the f_t, and the weight alpha_t.

    - Gradient Boost vs AdaBoosting difference
        Gradient Boost is more generic,
                     while AdaBoosting uses exponential function in loss.

    pros and cons:
        - pros: generally performs better than RF
        - cons: sensitive to outliers 
                                    because the result is dependent upon previous errors
-- hyperplane --

- SVM/SVR
    idea: 
        SVM: producing a hyperplan that seperates the dataset,
             support vectors are those on or inside the margin.
        SVR: producing a line, supported by a eposilon tube, fitting the dataset,
             support vectors are those on or outside the margin (eposilon tube).

    details to note:
        mathematical details is first to write the problem as constrained optimization,
        by adding constrained equality to the optimization equation (Lagrangian).
        Then solve the problem by primal(min-max) then recover the dual(max-min) optimization.
    
    pros and cons:
        pros: memory efficient: only need to store support vectors
              works well when d>n

        cons: if the dataset is large, the training time is very long (O(n**3)).
              fails when having many noise



- Unsupervised: Matrix factorization, PCA, Kmeans, hierarchical
     Kmeans:
		Pros: efficient, running time is O(N*t*d*k), where t is #iterations, d is dimension, and k is #clusters. 
		Cons: need to run many times to get numbers of clusters (elbow method)

     Hierarchical Clustering (a type of agglomerative clustering):
		Pros: can get numbers of clusters through dendrogram (AKA just run once) 
		Cons: running time is O(N**3)

- Deep learning: MLP, CNN
    MLP: Forward: each layer has many neurons which inplace a linear function of the input and then later an
         activation function (ReLu, sigmoid, tanh). the final layer is the output
         Backward: gradient descent to calibrate weight
    CNN: having many sets of CNN layers and maxpool layers before FC layer
         ideas: padding, stride

- Other idea: Kernel
    idea: mapping lower dimension matrix to higher dimension matrix using a function


- Model type summary:
====================================================================================================
			Generative(used Bayesian rule)		Discriminative(directly find Pr(y|X))
----------------------------------------------------------------------------------------------------
Parametric		Discriminative Analysis (LDA, QDA)	Logistic Regression
(has functional form)	Naive Bayes				Neural Network
(#param doesn't change)		
----------------------------------------------------------------------------------------------------
Non-Parametric							KNN, Tree-based Method (RF), 
								Generative Additive model (GAM)
====================================================================================================


    

Part II. data modeling techniques

Data preprocessing:
- handling missing value
   - imputation: KNN, forward or backward filling after sorting
   - replace with mean/median/mode
   - delete
- handling outlier
   - detection: distance based (nearest neighbor) or density based (LOF)
   - against: robust regression

Modeling:
- Evaluation Metric
    - classification: 
        - accuracy
        - precision TP / (TP + FP)
        - recall TP / (TP + FN)
        - F1: score combination of precision and recall
        - ROC curve (TP vs FP) and AUC (0~1)
    - Regression: 
        - Rooted Mean Square Error: lower better
        - R^2 (0~1): higher better

- handling imbalance dataset:
    - way1: add class weight in loss function
    - way2: generate noise for the minority class (SMOTE)

- handling time series dataset: 
    - way1: generate information about changes in time, and use traditional ML model
    - way2: use time series model or DL model having memorizing unit (LSTM, RNN, etc)
