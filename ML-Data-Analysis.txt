Part I. ML methods


- Linear Regression & Penalized Linear Regression
    idea: OLS: objective is to minimize the sum of squared errors
          variants: handling outliers: Robust linear regression
                    overfitting: L1 (Lasso) Regression, L2 (Ridge) Regression
    details to note: 
          L1 (Lasso) could be used as feature selection since it can shrink coeffiencts to 0
          L2 is usually better than L1 in terms of performance
    pros and cons:
          pros: linear regression: can be used as baseline model
                Interpretable


- Logistic Regression
    idea: transform a binary outcome to a continuous probability


- KNN
    idea: memorize the entire training set, and then for a test point find its k-nearest  neighbor and take
          the majority vote
    
    pros and cons:
        pros: simple and no assumptions about the data
        cons: memory inefficient: need to store the entire training set
	      often fails if dimension (d) is large

- Naive Bayes
    idea: use Bayes rules to find posterior probability distribution
	  derive the class conditional distribution through multiplication of probs in training sample

    pros and cons: 
        pros: works well in high dimension (such as text classification)
        cons: assumes independence of each predictors

- Linear Discriminant analysis
    idea: use Bayes rules to find posterior probability distribution
	  derive the class conditional distribution through Gaussian distribution (so it becomes the assumption of distribution of training data)


-- Tree-based model--

- Decision Tree:
    idea: splitting data by attribute and threshold at each node. 
	  attribute and threshold are selected by maximized information gain, which is the change of impurity after one split
	  tree pruning: minimizing the total impurity in each node
	  impurity measure: could be Entropy or Gini Index
    pros: easy to interpetate 
 
	
- Random Forest: (ensemble method)
    idea: 
    for each split, only take a random subset of features to form a partition （this where the randomness comes in）
    take average of prediction outcomes from a set of DTs:
            - regression: average
            - classification: majority vote
    pros and cons:
        - pros: generally good performance due to not correlated trees
        - cons: could be black-box model

    
- Boosting: (ensemble method)
	idea:
        trees and weights are learned sequentially (samples are downweighted or unweighted )
	final vote is * weighted average


    details:
        - downweight: if samples are weighted correctly
          upweight: if samples are weighted incorrectly
        - a set of weak learners that collaboratively becomes an effective learner after ensembling

    - Gradient Boost:
        To avoid overfitting on gradient, a new weaker learner f_t is introduced:
            The f_t is added to the negative of the gradient of F_t-1 to get the loss.
            Minimize the loss to get the f_t, and the weight alpha_t.

    - Gradient Boost vs AdaBoosting difference
        Gradient Boost is more generic,
                     while AdaBoosting uses exponential function in loss.

    pros and cons:
        - pros: generally performs better than RF
        - cons: sensitive to outliers, because the result is dependent upon previous errors
		can lead to overfit, since bias is decreased each iteration. Typically stop early is a solution

-- hyperplane --

- SVM/SVR
    -idea: 
        SVM: producing a hyperplan that seperates the dataset.
	     the goal is to maximize the minimum distance (or the margin) between data and hyperplane
             support vectors are those on or inside the margin.
        SVR: producing a line, supported by a eposilon tube, fitting the dataset,
             support vectors are those on or outside the margin (eposilon tube).

    -details to note:

	We can solve the problem by dual(max-min) and then recover to primal(min-max), to 1) kernalize SVM to produce non-linear 
	boundary; 2) efficiently compute the result when d >> n.
        mathematical details can be viewed in the pdf file.
 
    -pros and cons:
        pros: memory efficient: only need to store support vectors
              works well when d>n

        cons: if the dataset is large, the training time is very long (O(n**3)).
              fails when having many noise



- Unsupervised: dimension reduction(PCA..), Kmeans, hierarchical
     - PCA:	the objective is to find a direction (unit vector) 
		such that the variance of data is maximized in this direction.

		details: can use SVD to solve the problem; directly calculate covariance matrix could be costly
		usage: dimension reduction (construction of new feature space)
			data visualization 
     -Kmeans:
		Pros: efficient, running time is O(N*t*d*k), where t is #iterations, d is dimension, and k is #clusters. 
		Cons: need to run many times to get numbers of clusters (elbow method)

     -Hierarchical Clustering (a type of agglomerative clustering):
		Pros: can get numbers of clusters through dendrogram (AKA just run once) 
		Cons: running time is O(N**3)

- Deep learning: MLP, CNN
    -MLP: Forward: each layer has many neurons which inplace a linear function of the input and then later an
         activation function (ReLu, sigmoid, tanh). the final layer is the output
         Backward: gradient descent to calibrate weight
    -CNN: having many sets of CNN layers and maxpool layers before FC layer
         ideas: padding, stride

- Other idea:
    Kernel Trick
    	-idea: 
	  	mapping lower dimension matrix to higher dimension matrix using a function including cross product of X
	  	Instead of transform entire dataset to higher dimension, just using cross product of X will produce the result
	    	e.g. K(x,x') = (1 + <x, x'>)**2  <=> K(x, x')= <b(x), b'(x)>, where b(x)=(1, sqrt(x1), sqrt(x2), x1^2, x2^2, sqrt(2)x1x2)
    -use case: 
	 	SVM with kernel
         	kernelized ridge regression
		kernelized PCA



- Model type summary:
====================================================================================================
			Generative(used Bayesian rule)		Discriminative(directly find Pr(y|X))
----------------------------------------------------------------------------------------------------
Parametric		Discriminative Analysis (LDA, QDA)	Logistic Regression
(has functional form)	Naive Bayes				Neural Network
(#param doesn't change)		
----------------------------------------------------------------------------------------------------
Non-Parametric							KNN, Tree-based Method (RF), 
								Generative Additive model (GAM)
====================================================================================================


    

Part II. data modeling techniques

Data preprocessing:

- Exploratory data analysis (EDA)
  - find correlation between features (in clf) or feature with outcome (in regression)   
  - variable distribution (Kernel Density Estimation for continuous, Boxplot for categorical)

- handling missing value
   - imputation: KNN, forward or backward filling after sorting
   - replace with mean/median/mode
   - delete

- handling outlier
   - detection: distance based (nearest neighbor) or density based (LOF)
		easier way: mean +- 3 sd
			    regression: statistical test :internal studentized deviation 
   - against: robust regression

Modeling:
- variable selection:
	by correlation;
	fit a model first: lasso(automate selection), or RF (manually find variable importance)

- model selection:
	based on bias vs variance trade off

- Evaluation Metric
    - classification: 
        - accuracy
        - precision TP / (TP + FP)
        - recall TP / (TP + FN)
        - F1: score combination of precision and recall
        - ROC curve (TP vs FP) and AUC (0~1)
    - Regression: 
        - Rooted Mean Square Error (sort of averaged RSS): lower better
        - R^2 (RSS/TSS) (0~1): higher better

- handling imbalance dataset:
    - way1: add class weight in loss function
    - way2: generate noise for the minority class (SMOTE)
    - way3: if does not care about accuracy too much, can switch performance measure

- handling time series dataset: 
    - way1: generate information about changes in time, and use traditional ML model
    - way2: use time series model or DL model having memorizing unit (LSTM, RNN, etc)
